{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-06 07:49:14.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mProject version: 2.7.2\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-06 07:49:17.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.utils.config\u001b[0m:\u001b[36mconfig\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mRUNNING WITH ARGS: netuid=None wallet.name=None wallet.hotkey=None subtensor.network=None axon.port=None\u001b[0m\n",
      "\u001b[32m2024-09-06 07:49:17.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.utils.config\u001b[0m:\u001b[36mconfig\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mUNKNOWN ARGS: ['--f=/root/.local/share/jupyter/runtime/kernel-v2-904744jocVEzQNB1yR.json']\u001b[0m\n",
      "\u001b[32m2024-09-06 07:49:17.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.settings\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mConfig: \n",
      "netuid: null\n",
      "wallet:\n",
      "  name: null\n",
      "  hotkey: null\n",
      "subtensor:\n",
      "  network: null\n",
      "axon:\n",
      "  port: null\n",
      "no_prompt: false\n",
      "config: null\n",
      "strict: false\n",
      "no_version_checking: false\n",
      "\u001b[0m\n",
      "\u001b[32m2024-09-06 07:49:17.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.utils.config\u001b[0m:\u001b[36mconfig\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mRUNNING WITH ARGS: netuid=None wallet.name=None wallet.hotkey=None subtensor.network=None axon.port=None\u001b[0m\n",
      "\u001b[32m2024-09-06 07:49:17.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.utils.config\u001b[0m:\u001b[36mconfig\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mUNKNOWN ARGS: ['--f=/root/.local/share/jupyter/runtime/kernel-v2-904744jocVEzQNB1yR.json']\u001b[0m\n",
      "\u001b[32m2024-09-06 07:49:17.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.settings\u001b[0m:\u001b[36mload_env\u001b[0m:\u001b[36m150\u001b[0m - \u001b[1mInstantiating bittensor objects with NETUID: 61, WALLET_NAME: test_wallet, HOTKEY: test_hotkey\u001b[0m\n",
      "\u001b[32m2024-09-06 07:49:18.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.settings\u001b[0m:\u001b[36mload_env\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mBittensor objects instantiated... WALLET: wallet(test_wallet, test_hotkey, ~/.bittensor/wallets/), SUBTENSOR: subtensor(test, wss://test.finney.opentensor.ai:443/), METAGRAPH: metagraph(netuid:61, n:226, block:2738986, network:test)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Initialise the settings\n",
    "from prompting import settings\n",
    "settings.settings = settings.Settings(mode=\"validator\")\n",
    "settings = settings.settings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/prompting/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 07:49:19,667\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\u001b[32m2024-09-06 07:49:19.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.llms.utils\u001b[0m:\u001b[36mcalculate_single_gpu_requirements\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mAvailable free memory: 84.53 GB\u001b[0m\n",
      "\u001b[32m2024-09-06 07:49:19.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.llms.utils\u001b[0m:\u001b[36mcalculate_single_gpu_requirements\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mTotal gpu memory 84.97 GB\u001b[0m\n",
      "\u001b[32m2024-09-06 07:49:19.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.llms.utils\u001b[0m:\u001b[36mcalculate_single_gpu_requirements\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m24.0% of the GPU memory will be utilized for loading the model to device \"CUDA\".\u001b[0m\n",
      "\u001b[32m2024-09-06 07:49:19.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.llms.vllm_llm\u001b[0m:\u001b[36mload_vllm_pipeline\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoading VLLM pipeline with model_id casperhansen/llama-3-8b-instruct-awq: Max. VRAM: 0.24; GPUs: 1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-06 07:49:19 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 09-06 07:49:19 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/llama-3-8b-instruct-awq', speculative_config=None, tokenizer='casperhansen/llama-3-8b-instruct-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/llama-3-8b-instruct-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 09-06 07:49:20 model_runner.py:680] Starting to load model casperhansen/llama-3-8b-instruct-awq...\n",
      "INFO 09-06 07:49:20 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  5.81it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.77it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.96it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-06 07:49:22 model_runner.py:692] Loading model weights took 5.3453 GB\n",
      "INFO 09-06 07:49:23 gpu_executor.py:102] # GPU blocks: 6329, # CPU blocks: 2048\n"
     ]
    }
   ],
   "source": [
    "# Initialise the LLM we use on the validator\n",
    "from prompting.llms.vllm_llm import vLLMPipeline\n",
    "pipeline = vLLMPipeline(llm_model_id=\"casperhansen/llama-3-8b-instruct-awq\", llm_max_allowed_memory_in_gb=20, device=\"CUDA\", quantization=False, llm_max_model_len=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.24s/it, est. speed input: 2.17 toks/s, output: 79.19 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' * 1+2 is 3.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you can see, \"pipeline\" is an object that simply wraps around the LLM and is callable\n",
    "pipeline(\"What's 1+2?\").split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Datasets generate 'Context' objects, which contain a 'row' of data, in this case about wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Context(title='Eightmile Island', topic='Contents', subtopic='Eightmile Island is a forested island on the Ohio River in Mason County, West Virginia. The island is located directly across the river from the village of Cheshire, Ohio and American Electric Power Company power plant facilities there.\\n', content='Eightmile Island is a forested island on the Ohio River in Mason County, West Virginia. The island is located directly across the river from the village of Cheshire, Ohio and American Electric Power Company power plant facilities there.\\n', internal_links=[], external_links=['American Electric Power Company', 'Cheshire, Ohio', 'Island', 'Ohio', 'Ohio River', 'Power plant', 'Mason County, West Virginia', 'West Virginia', 'List of islands of West Virginia', 'Geographic coordinate system'], source='Wikipedia', tags=['Coordinates on Wikidata', 'Infobox mapframe without OSM relation ID on Wikidata', 'Islands of the Ohio River', 'Landforms of Mason County, West Virginia', 'Metro Valley geography stubs', 'River islands of West Virginia'], extra={'url': 'https://en.wikipedia.org/wiki/Eightmile_Island', 'page_length': 72, 'section_length': 38}, stats=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prompting.datasets.wiki import WikiDataset\n",
    "dataset = WikiDataset()\n",
    "context = dataset.random()\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Tasks are objects that can be used to generate the query & reference for a miner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise with past data\n",
    "\n",
    "We can either initialise the task with past data (this doesn't require an LLM to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-06 07:49:34.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.tasks.base_task\u001b[0m:\u001b[36mgenerate_query\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1m🤖 Generating query...\u001b[0m\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.27s/it, est. speed input: 19.61 toks/s, output: 78.46 toks/s]\n",
      "\u001b[32m2024-09-06 07:49:37.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.llms.vllm_llm\u001b[0m:\u001b[36m_forward\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mvLLM_LLM generated the following output:\n",
      "\n",
      "\n",
      "To generate a request about Eight Mile Island, I'd like to request a detailed summary of the topic. Here's a breakdown of what I'd like to know:\n",
      "\n",
      "**Background Information**: \n",
      "Please provide a brief overview of Eight Mile Island, including its geographical location and any notable features. For instance, is it an island in a specific country or body of water? Are there any notable landmarks, such as lighthouses or beaches?\n",
      "\n",
      "**Historical Significance**: \n",
      "I'd like to know about any significant historical events or milestones that have occurred on or around Eight Mile Island. This could include notable shipwrecks, pirate sightings, or historical events that have impacted the island's development.\n",
      "\n",
      "**Environmental Characteristics**: \n",
      "Please describe the natural environment of Eight Mile Island, including its vegetation, wildlife, and climate. Are there any unique or endangered species found on the island? How does the island's ecosystem interact with its surrounding environment?\n",
      "\n",
      "**Human Settlement and Development**: \n",
      "Can you provide information about the human history of Eight Mile Island? Has the island been inhabited by indigenous peoples, or has it been settled by European colonizers or modern-day settlers? Are there any notable buildings, structures, or infrastructure on the island?\n",
      "\n",
      "**Conservation Efforts and Challenges**: \n",
      "I'd\u001b[0m\n",
      "\u001b[32m2024-09-06 07:49:37.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.tasks.base_task\u001b[0m:\u001b[36mgenerate_reference\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1m🤖 Generating reference...\u001b[0m\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, est. speed input: 188.83 toks/s, output: 78.19 toks/s]\n",
      "\u001b[32m2024-09-06 07:49:38.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.llms.vllm_llm\u001b[0m:\u001b[36m_forward\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mvLLM_LLM generated the following output:\n",
      "\n",
      "\n",
      "Here is a concise and accurate summary of the context:\n",
      "\n",
      "Eightmile Island is a forested island located on the Ohio River in Mason County, West Virginia, opposite the village of Cheshire, Ohio, and the American Electric Power Company's power plant facilities.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"\\n\\nTo generate a request about Eight Mile Island, I'd like to request a detailed summary of the topic. Here's a breakdown of what I'd like to know:\\n\\n**Background Information**: \\nPlease provide a brief overview of Eight Mile Island, including its geographical location and any notable features. For instance, is it an island in a specific country or body of water? Are there any notable landmarks, such as lighthouses or beaches?\\n\\n**Historical Significance**: \\nI'd like to know about any significant historical events or milestones that have occurred on or around Eight Mile Island. This could include notable shipwrecks, pirate sightings, or historical events that have impacted the island's development.\\n\\n**Environmental Characteristics**: \\nPlease describe the natural environment of Eight Mile Island, including its vegetation, wildlife, and climate. Are there any unique or endangered species found on the island? How does the island's ecosystem interact with its surrounding environment?\\n\\n**Human Settlement and Development**: \\nCan you provide information about the human history of Eight Mile Island? Has the island been inhabited by indigenous peoples, or has it been settled by European colonizers or modern-day settlers? Are there any notable buildings, structures, or infrastructure on the island?\\n\\n**Conservation Efforts and Challenges**: \\nI'd\",\n",
       " \"\\n\\nHere is a concise and accurate summary of the context:\\n\\nEightmile Island is a forested island located on the Ohio River in Mason County, West Virginia, opposite the village of Cheshire, Ohio, and the American Electric Power Company's power plant facilities.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prompting.tasks.summarization import SummarizationTask, SummarizationRewardConfig\n",
    "SummarizationTask.generate_query_reference(llm_pipeline=pipeline, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miner Responses\n",
    "\n",
    "Now let's say we have a few miners giving us responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from prompting.base.dendrite import DendriteResponseEvent, SynapseStreamResult, StreamPromptingSynapse\n",
    "\n",
    "miner_response_1 = SynapseStreamResult(synapse=StreamPromptingSynapse(completion=\"4\", roles=[\"user\"], messages=[\"What's 1+2?\"]))\n",
    "miner_response_2 = SynapseStreamResult(synapse=StreamPromptingSynapse(completion=\"3\", roles=[\"assistant\"], messages=[\"What's 1+2?\"]))\n",
    "\n",
    "\n",
    "# the synapses from all miners get collected into the DenriteResponseEvent\n",
    "dendrite_response = DendriteResponseEvent(stream_results=[miner_response_1, miner_response_2], uids=np.array([1, 2]), timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring\n",
    "\n",
    "We can now pass the query, reference and miner responses to our scoring function, which is then responsible for giving each miner a score which is later used to set weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17160807, 0.39458477])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prompting.tasks.summarization import SummarizationRewardConfig\n",
    "\n",
    "reward_events, penality_events, rewards = SummarizationRewardConfig.apply(challenge=\"What's 1+2?\", reference=\"1+2 is equal to 3\", response_event=dendrite_response)\n",
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other tests/examples on different tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompting.tasks.qa import QuestionAnsweringTask, QARewardConfig\n",
    "qa = QuestionAnsweringTask(context=context.model_dump(), llm_pipeline=pipeline, reward_config=SummarizationRewardConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to obtain the query (which is a question about the context)\n",
    "QUERY_PROMPT_TEMPLATE = \"\"\"\\\n",
    "Ask a specific question about the following context:\n",
    "\n",
    "#Context:\n",
    "{context}\n",
    "\n",
    "You must ask a question that can be answered by the context.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-06 07:49:38.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.tasks.base_task\u001b[0m:\u001b[36mgenerate_query\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1m🤖 Generating query...\u001b[0m\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.97it/s, est. speed input: 439.14 toks/s, output: 50.78 toks/s]\n",
      "\u001b[32m2024-09-06 07:49:39.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.llms.vllm_llm\u001b[0m:\u001b[36m_forward\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mvLLM_LLM generated the following output:\n",
      "\n",
      "\n",
      "What is the location of Eightmile Island in relation to the Ohio River?\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhat is the location of Eightmile Island in relation to the Ohio River?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_prompt = QUERY_PROMPT_TEMPLATE.format(context=context.content)\n",
    "query = qa.generate_query(llm_pipeline=pipeline, message=query_prompt)\n",
    "query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to obtain reference answer\n",
    "REFERENCE_PROMPT_TEMPLATE = \"\"\"\\\n",
    "Answer the question you will receive in detail, utilizing the following context.\n",
    "\n",
    "#Context:\n",
    "{context}\n",
    "\n",
    "# Question:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-06 07:49:39.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.tasks.base_task\u001b[0m:\u001b[36mgenerate_reference\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1m🤖 Generating reference...\u001b[0m\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.09s/it, est. speed input: 73.47 toks/s, output: 77.31 toks/s]\n",
      "\u001b[32m2024-09-06 07:49:41.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.llms.vllm_llm\u001b[0m:\u001b[36m_forward\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mvLLM_LLM generated the following output:\n",
      "\n",
      "\n",
      "Based on the provided context, Eightmile Island is located on the Ohio River in Mason County, West Virginia. This means that the island is situated directly in the Ohio River, with a unique characteristic that it is located directly across the river from the village of Cheshire, Ohio, and the American Electric Power Company power plant facilities there.\n",
      "\n",
      "To be more specific, Eightmile Island is not located on the banks of the Ohio River, but rather it is a forested island that is already situated in the river itself. This makes it a distinctive and isolated geographical feature, with the river flowing around it. The proximity to the village of Cheshire, Ohio, and the power plant facilities also highlights the island's strategic location, which is likely to have played a role in its historical significance or usage.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nBased on the provided context, Eightmile Island is located on the Ohio River in Mason County, West Virginia. This means that the island is situated directly in the Ohio River, with a unique characteristic that it is located directly across the river from the village of Cheshire, Ohio, and the American Electric Power Company power plant facilities there.\\n\\nTo be more specific, Eightmile Island is not located on the banks of the Ohio River, but rather it is a forested island that is already situated in the river itself. This makes it a distinctive and isolated geographical feature, with the river flowing around it. The proximity to the village of Cheshire, Ohio, and the power plant facilities also highlights the island's strategic location, which is likely to have played a role in its historical significance or usage.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_prompt = REFERENCE_PROMPT_TEMPLATE.format(context=context.content, question=query)\n",
    "reference = qa.generate_reference(llm_pipeline=pipeline, messages=[reference_prompt])\n",
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompting-fb5sw-i7-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
